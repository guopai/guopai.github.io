<!doctype html>
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-101718744-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-101718744-3');
</script>

<meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="generator" content="ReText 7.0.4">
<title>Machine Learning บทที่ 2: Linear Regression Algorithm</title>
<meta name="description" content="Machine learning อธิบาย Linear regression algorithm: Hypothesis function, Cost function, Gradient descent">
<meta name="keywords" content="Machine learning, Deep learning, AI">
<meta name="author" content="ชิตพงษ์ กิตตินราดร">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  config: ["MMLorHTML.js"],
  jax: ["input/TeX", "input/AsciiMath", "output/HTML-CSS", "output/NativeMML"],
  extensions: ["MathMenu.js", "MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js", "AMSsymbols.js"],
    equationNumbers: {autoNumber: "AMS"}
  }
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js"></script></head>
<body>
<h1>Linear Regression Algorithm</h1>
<p>โดย <a href="about.html">ชิตพงษ์ กิตตินราดร</a> | ธันวาคม 2562</p>
<p>วิธีที่ดีที่สุดในการทำความเข้าใจ Machine learning คือการอธิบายพร้อมกับตัวอย่าง โดยเราจะเริ่มที่ Algorithm ที่ง่ายที่สุด นั่นคือ Linear regression</p>
<p><img alt="Alt text" src="images/ml-blog02-01.png"></p>
<p>เราจะใช้ข้อมูลตัวอย่างจาก <a href="https://www.kaggle.com/venjktry/simple-linear-regression/data">Kaggle</a> ซึ่งแสดงความสัมพันธ์ระหว่างตัวแปร x บนแกนนอนกับตัวแปร y บนแกนตั้ง ถ้ามองด้วยตาเราจะเห็นว่า x กับ y มีความสัมพันธ์ที่เรียกว่า Positive correlation อย่างชัดเจน นั่นคือยิ่ง  x มีค่ามาก y ก็มีค่ามากตามไปด้วย เราจะเห็นความสัมพันธ์ลักษณะในโลกจริงได้มากมาย ตัวอย่างเช่น ยิ่งจำนวนรถมาก เวลาที่ใช้บนถนนต่อการเดินทางหนึ่งเที่ยวก็มีแนวโน้มจะมากขึ้นด้วย</p>
<p>ปัจจุบัน ชุดข้อมูลนี้มีค่า x และ y จำนวน 999 คู่ ตัวอย่างเช่น คู่แรก x = 24, y = 21.54 ส่วนคู่ที่สอง x = 50, y = 47.46 เราเรียกข้อมูลแต่ละคู่เช่นนี้ว่าข้อมูลหนึ่งรายการ</p>
<p>จริงอยู่ที่เรามีข้อมูลถึง 999 รายการ แต่ถ้าเราเก็บข้อมูลมาใหม่เป็นรายการที่ 1000 โดยข้อมูลรายการนี้รู้แค่ค่า x เราจะรู้ได้อย่างไรว่าค่า y เป็นเท่าไร</p>
<p>ตัวอย่างเช่น ถ้าข้อมูลใหม่มี x = 22 เราอาจจะลองลากเส้นแนวดิ่งจากแกน x ที่ 22 แล้วไปบรรจบตั้งฉากกับแกน y ซึ่งดูเหมือนจะได้ประมาณ 20 แต่ช้าก่อน มันอาจจะเป็น 18, 25 หรือแม้กระทั่ง 30 ก็ได้ เพราะข้อมูลตัวอย่างของเรามีรายการที่ x มีค่าประมาณ 22 อยู่หลายรายการ ซึ่งแต่ละรายการก็มีค่า y ไม่เท่ากันเลย</p>
<p>Machine learning จึงมาช่วยเราได้ โดยการใช้ Algorithm ซึ่งก็คือสูตรและกลไกทางคณิตศาสตร์ มาเรียนรู้ข้อมูลที่เรามี เพื่อสร้างโมเดลของข้อมูลขึ้นมา เราจะใช้โมเดลที่สร้างขึ้นนี้ ในการพยากรณ์คำตอบ ซึ่งก็คือค่า y ที่เราต้องการ</p>
<p>ซึ่งในกรณีนี้ โมเดลที่ได้คือ:</p>
<p>
<script type="math/tex; mode=display">\hat{y} = 1.00065638x - 0.10726546\tag{1}</script>
</p>
<p>ดังนั้นถ้าเราลองใส่ค่า x = 22 ลงไป จะได้ y = 21.90</p>
<p>คำถามคือ โมเดลนี้ได้มาอย่างไร คำตอบอยู่ในส่วนต่อไป</p>
<h2>Hypothesis function</h2>
<p>หัวใจของโมเดลพยากรณ์ คือฟังก์ชันคณิตศาสตร์ที่เป็นตัวแทนความสัมพันธ์ระหว่างตัวแปร x กับคำตอบ y เราเรียกฟังก์ชันนี้ว่า Hypothesis function ซึ่งหมายถึงฟังก์ชันที่เป็น "สมมุติฐาน" ว่าข้อมูลมีความสัมพันธ์กันอย่างไรนั่นเอง โดยในปัญหา Linear regression นั้น Hypothesis function จะอยู่ในรูปแบบ:</p>
<p>
<script type="math/tex; mode=display">\hat{y} = wx + b\tag{2}</script>
</p>
<ul>
<li>
<script type="math/tex">\hat{y}</script> คือค่าคำตอบที่พยากรณ์ได้</li>
<li>
<script type="math/tex">w</script> คือค่าน้ำหนักของตัวแปร x (มาจากคำว่า Weight) ซึ่งกำหนดความชันของฟังก์ชัน</li>
<li>
<script type="math/tex">x</script> คือตัวแปร x ที่เรารู้ค่า</li>
<li>
<script type="math/tex">b</script> คือจุดตัดแกน y (มาจากคำว่า Bias)</li>
</ul>
<p>ฟังก์ชันนี้อยู่ในรูปแบบสมการเส้นตรง นั่นคือค่า x ที่ใส่ลงไป จะมีความสัมพันธ์กับค่า y แบบที่ลากเป็นเส้นตรงได้</p>
<p>หน้าที่ของ Machine learning คือการหาตัวแปร w และ b ซึ่งจะประกอบกันเป็น Hypothesis function ที่มีความคลาดเคลื่อนน้อยที่สุดเมื่อเทียบกับข้อมูลที่เรามี ซึ่งในกรณีตัวอย่างนี้ เราได้ w = 1.00065638 และ b = -0.10726546</p>
<p>เรารู้ได้อย่างไรว่า w และ b ต้องมีค่าเท่าใดจึงจะทำให้โมเดลมีความคลาดเคลื่อนน้อยที่สุด คำตอบมีอยู่ 2 ส่วน ส่วนแรกคือเราต้องมีวิธีการวัดความคลาดเคลื่อน ซึ่งเรียกว่า Cost function ส่วนที่สองคือเราต้องมีวิธีการหาค่า w และ b ที่จะทำให้ค่าความคลาดเคลื่อนใน Cost function นั้นต่ำที่สุด เรียกกระบวนการนี้ว่า Gradient descent</p>
<h2>Cost function</h2>
<p>สำหรับ Hypothesis function แต่ละฟังก์ชัน จะมี Cost function คู่อยู่เสมอ ซึ่งทำหน้าที่วัดว่าค่า y ที่พยากรณ์ได้จาก Hypothesis function มีความคลาดเคลื่อนกับค่า y ที่เรามีข้อมูลอยู่จริงเท่าใด โดยสำหรับ Linear regression เราใช้ฟังก์ชันต่อไปนี้เป็น Cost function:</p>
<p>
<script type="math/tex; mode=display">J(w, b) = \frac{1}{2m} \sum\limits_{i = 1}^m(\hat{y}_i - y_i)^2\tag{3}</script>
</p>
<ul>
<li>
<script type="math/tex">J(w, b)</script> คือผลความต่างระหว่างค่าพยากรณ์กับค่าจริง ที่สอดคล้องกับตัวแปร w และ b</li>
<li>
<script type="math/tex">m</script> คือจำนวนรายการข้อมูลทั้งหมดที่มี (เช่นในกรณีตัวอย่างนี้ m = 699 มาจากข้อมูล 999 รายการ แต่กัน 300 รายการเอาไว้ทดสอบความแม่นยำ เรื่องนี้จะอธิบายภายหลัง)</li>
<li>
<script type="math/tex">\sum\limits_{i = 1}^m</script> คือผลรวมของรายการถัดไป โดยรวมจากรายการที่ 1 ถึงรายการที่ m</li>
<li>
<script type="math/tex">(\hat{y}_i - y_i)^2</script> คือผลต่างระหว่างค่า y ที่พยากรณ์ได้ กับค่า y จริง ของแต่ละรายการ แล้วนำไปยกกำลัง 2 ตัวอย่างเช่น ถ้ารายการแรก พยากรณ์ได้ 20 แต่ในความเป็นจริงต้องเป็น 21.54 จะได้ผลต่างยกกำลัง 2 เท่ากับ <script type="math/tex">(20-21.54)^2 = 2.37</script>
</li>
</ul>
<p>อนึ่ง ฟังก์ชันนี้ มีชื่อเรียกเฉพาะตัว ว่า Mean Squared Error หรือ MSE</p>
<p>เป้าหมายของ Algorithm คือการทำให้ <script type="math/tex">J(w, b)</script> มีค่าต่ำที่สุดเท่าที่จะเป็นไปได้ เราเรียกจุดนี้ว่า Global minimum ซึ่งหาได้โดยใช้กระบวนการที่เรียกว่า Gradient descent</p>
<h2>Gradient descent</h2>
<p>ทบทวนอีกครั้งว่าเรามี Cost function ที่กำหนดความสัมพันธ์ระหว่างตัวแปร w, b กับความคลาดเคลื่อนของโมเดล ในโมเดลที่ข้อมูลมีมิติเดียว (คือมี x เพียงหนึ่งตัวสำหรับข้อมูลแต่ละรายการ แต่ละ x ก็มี w เพียงตัวเดียวเป็นค่าน้ำหนัก) รูปร่างของ Cost function จะมีลักษณะเป็นเหมือนชาม คือมีจุดต่ำที่สุด ซึ่งจุดนั้นเป็นจุดที่เราต้องการเนื่องจากจะทำให้ได้ค่าความคลาดเคลื่อนที่ต่ำที่สุด หน้าที่ของ Gradient descent คือการหาตัวแปร w และ b ที่จะทำให้ได้ J ที่ต่ำที่สุด</p>
<p><img alt="Alt text" src="images/ml-blog02-03.png"></p>
<p>ในที่นี้เราจะพิจารณาเฉพาะ Cost function ของ <script type="math/tex">J(w)</script> ซึ่งผันแปรตามตัวแปร w เพียงตัวเดียวก่อน</p>
<p>การทำงานของ Gradient descent ใช้ประโยชน์จากข้อเท็จจริงที่ว่า ในแต่ละจุดของเส้นฟังก์ชัน <script type="math/tex">J(w)</script> เราจะสามารถลากเส้นไปแตะเพื่อชี้วัด "ความชัน" ของจุดนั้นๆ ได้ จากภาพเราจะเห็นว่าจุดที่ 1 มีความชันมากกว่าจุดที่ 2 และจุดที่ 2 ก็ชันมากกว่าจุดที่ 3 จากคุณสมบัตินี้ หากเรามีกลไกที่จะหาจุดที่มีความชันน้อยลงเรื่อยๆ ไปจนถึงจุดที่แทบไม่มีความชันเลย (จุดที่ 3) เราก็จะได้ Global minimum ที่เราต้องการ</p>
<p>การหาความชันทำได้ด้วยวิชา Calculus โดยการหาอนุพันธ์ (Derivative) ของฟังก์ชัน <script type="math/tex">J(w)</script> เมื่อเปรียบเทียบกับ w ซึ่งแทนเป็นสัญลักษณ์ได้ด้วย:</p>
<p>
<script type="math/tex; mode=display">\frac{\partial J(w)}{\partial w}\tag{4}</script>
</p>
<p>จากวิชา Calculus เราหาอนุพันธ์ของฟังก์ชันดังกล่าว จะได้ Algorithm ทั่วไปคือ:</p>
<p><em>ทำซ้ำจนกระทั่งผลลัพธ์ล่าสุดไม่เปลี่ยนแปลงจากผลลัพธ์ครั้งก่อน:</em></p>
<p>
<script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta_0, \theta_1)\tag{5}</script>
</p>
<ul>
<li>
<script type="math/tex">\theta</script> อ่านว่า Theta คือตัวแปรต่างๆ ของโมเดล เช่น b, w</li>
<li>
<script type="math/tex">j</script> คือลำดับที่ของตัวแปร โดยจุดตัดแกน y หรือ b อยู่ลำดับที่ 0 ส่วน w อยู่ลำดับที่ 1</li>
<li>
<script type="math/tex">\alpha</script> อ่านว่า Alpha คือค่าคงที่ที่กำหนดอัตราความเร็วของการเคลื่อนที่ไปสู่จุดที่ความชันน้อยลง</li>
</ul>
<p>จาก Algorithm ทั่วไปนี้ นำมาประยุกต์กับ Linear regression จะได้ว่า:</p>
<p><em>ทำซ้ำจนกระทั่งผลลัพธ์ล่าสุดไม่เปลี่ยนแปลงจากผลลัพธ์ครั้งก่อน:</em></p>
<p>
<script type="math/tex; mode=display">b := b - \alpha \frac{1}{m} \sum\limits_{i = 1}^m(\hat{y_i} - y_i)\tag{6}</script>
</p>
<p>
<script type="math/tex; mode=display">w := w - \alpha \frac{1}{m} \sum\limits_{i = 1}^m(\hat{y_i} - y_i)x_i)\tag{7}</script>
</p>
<p>มาลองทำความเข้าใจ Algorithm นี้ด้วยตัวอย่าง สมมุติว่าเราเริ่มคำนวนด้วยค่าตั้งต้นดังนี้: <script type="math/tex">\theta_0 = b = 2</script>; <script type="math/tex">\alpha = 0.5</script> ถ้าเราหาอนุพันธ์ครั้งแรกได้ 1 ครั้งที่สองได้ 0.7 และครั้งที่สามได้ 0.4 เราจะได้:</p>
<p>
<script type="math/tex; mode=display">2 := 2 - (0.5)1 = 1.5\tag{8}</script>
</p>
<p>
<script type="math/tex; mode=display">1.5 := 1.5 - (0.5)0.7 = 1.15\tag{9}</script>
</p>
<p>
<script type="math/tex; mode=display">1.15 := 1.15 - (0.5)0.4 = 0.95\tag{10}</script>
</p>
<p>จะเห็นว่า ผลลัพธ์ค่า b ที่ได้มาค่าน้อยลงเรื่อยๆ คือจาก 1.5 เป็น 1.15 เป็น 0.95 โดยผลลัพธ์ที่ได้แต่ละรอบจะส่งผลให้ความชันของ Cost function ลดลงเรื่องๆ ทำให้อนุพันธ์น้อยลงเรื่อยๆ จาก 1 เป็น 0.7 เป็น 0.4 เป็นต้น</p>
<p>เป้าหมายของกระบวนการ Gradient descent นี้คือการหาค่าตัวแปรของโมเดลที่ทำให้โมเดลมีผลลัพธ์ y ที่คลาดเคลื่อนกับค่า y ที่แท้จริงให้น้อยที่สุด นั่นคือเราจะได้โมเดลที่เป็นตัวแทนของชุดข้อมูลนี้ได้ดีที่สุดนั่นเอง</p>
<p>ถ้าสังเกตดีๆ จะเห็นว่า เราสามารถควบคุมกระบวนการนี้ได้ด้วยการกำหนดตัวแปรสองสิ่ง เราเรียกตัวแปรที่เราเป็นผู้กำหนดเองนี้ว่า Hyperparameter ซึ่งในกรณีนี้ ประกอบด้วย:</p>
<ul>
<li>
<script type="math/tex">N_{iteration}</script> จำนวนครั้งที่เราคำนวนและอับเดตค่าใน Algorithm เช่น 100 ครั้ง หมายความว่าจะมีการคำนวนเพื่อลดความชันลงเรื่อยๆ 100 รอบ โดยถ้าจำนวนครั้งนี้น้อยเกินไป Algorithm ก็จะยังคำนวนไปไม่ถึงจุดที่จะได้ Cost ที่ต่ำที่สุด แต่ถ้าจำนวนครั้งนี้มากเกินไป ก็จะเสียเวลาและพลังงานในการคำนวนมากโดยไม่จำเป็น เพราะรอบหลังๆ ที่ได้ทำให้ Cost function เปลี่ยนแปลง</li>
<li>
<script type="math/tex">\alpha</script> เรียกว่า Learning rate หรืออัตราความเร็วในการอับเดตค่าความชัน (อนุพันธ์ของ J เมื่อเทียบกับตัวแปร) ถ้า Learning rate มีค่ามาก จะทำให้ค่าความชันส่งผลไปยังค่าของตัวแปรมากขึ้น ทำให้โมเดลหาจุด Global minimum ได้เร็วขึ้น ในทางกลับกัน ถ้า Learning rate น้อย ตัวแปรก็จะเปลี่ยนแปลงช้าลง</li>
</ul>
<p>ทีนี้ดูเหมือนกับว่า ถ้าเรากำหนด Learning rate สูงๆ ก็จะทำให้ได้ค่า Global minimum ของ Cost function เร็วขึ้น อย่างไรก็ตามถ้าค่า Learning rate สูงเกินไป อาจจะทำให้โมเดลหา Global minimum ไม่เจอ เพราะในขั้นท้ายๆ ของ Gradient descent ตัวแปรอาจจะกระโดดเลยจุดต่ำสุดของ J ไปอีกด้านหนึ่ง เพื่อความเข้าใจลองดูภาพประกอบ:</p>
<p><img alt="Alt text" src="images/ml-blog02-04.png"></p>
<h2>Normal equation</h2>
<p>นอกจากวิธี Gradient descent แล้ว เรายังมีวิธีการหา Parameter <script type="math/tex">\theta</script> ที่ให้ Cost ที่ต่ำที่สุดอีกวิธี เราเรียกวิธีนี้ว่า Normal equation โดยเพียงใช้สมการ:</p>
<script type="math/tex; mode=display">\theta = (X^T X)^{-1} X^T y \tag{11}</script>
<p>Normal equation ทำงานได้ดีและเร็วเมื่อจำนวน Feature ไม่มาก ผู้สนใจสามารถหาอ่านวิธีการ Derive ให้ได้มาซึ่ง Normal equation นี้ด้วยตนเอง</p>
<p>ตอนนี้เรามีความรู้และเข้าใจขั้นตอนการสร้างโมเดล Linear regression แล้ว ตอนถัดไปเราจะใช้ตัวอย่างนี้มาสร้างโมเดลจริงกัน</p>
<p><a href="index.html">หน้าแรก</a> | <a href="ml-blog01.html">บทที่ 1 บทนำ</a> | <a href="ml-blog03.html">บทที่ 3 Linear Regression Programming</a></p>
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.

</body>
</html>
