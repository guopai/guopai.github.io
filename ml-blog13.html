<!doctype html>
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-101718744-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-101718744-3');
</script>

<meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="generator" content="ReText 7.0.4">
<title>Machine Learning บทที่ 13: Neural Network Algorithm</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  config: ["MMLorHTML.js"],
  jax: ["input/TeX", "input/AsciiMath", "output/HTML-CSS", "output/NativeMML"],
  extensions: ["MathMenu.js", "MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js", "AMSsymbols.js"],
    equationNumbers: {autoNumber: "AMS"}
  }
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js"></script></head>
<body>
<h1>Neural Network Algorithm</h1>
<p>โดย <a href="https://www.linkedin.com/in/chitpong-kittinaradorn/">ชิตพงษ์ กิตตินราดร</a> | มกราคม 2563</p>
<p>Artificial neural network เป็น Algorithm หลักของการเรียนรู้แบบ Deep learning ซึ่งเป็นแนวทางการเรียนรู้แบบหนึ่งของ Machine learning</p>
<p>หลายคนคงเคยได้ยินมาว่า Deep learning ในปัจจุบันนั้นมีความสามารถมาก เช่นสามารถพยากรณ์มะเร็งจากภาพ X-ray ได้แม่นยำกว่าแพทย์ สามารถแยกแยะหน้าคนว่าใครเป็นใคร สามารถอ่านป้ายจราจรจากภาพถ่าย แยกเสียงพูดออกจากเสียงดนตรี เล่นหมากล้อมชนะแชมป์โลก ไปจนถึงการสร้างภาพดาราที่ไม่มีตัวตนจริง จะเห็นว่าความสามารถเหล่านี้ล้วนมีจุดร่วมกันอยู่หนึ่งอย่าง นั่นคือข้อมูล Input ที่ป้อนเข้า Deep learning algorithm มักจะเป็นข้อมูลแบบที่ไม่มีโครงสร้าง หรือที่เรียกว่า Unstructured data เช่น ภาพ ข้อความ เสียง ในขณะที่ Machine learning algorithm ทั่วไปมักจะเหมาะสมกับข้อมูลแบบ Structured data ที่มีโครงสร้างเป็นตาราง</p>
<p>อะไรคือสิ่งที่ทำให้ Neural network สามารถเรียนรู้จากข้อมูลที่ไม่มีโครงสร้างได้ดีกว่าวิธีอื่นๆ คำตอบอยู่ที่โครงสร้างการเรียนรู้ (Learning architecture) ของ Neural network ต่างจากโครงสร้างการเรียนรู้ของ Machine learning algorithm แบบทั่วไป</p>
<p>เรามาลองทำความเข้าใจโครงสร้างของการเรียนรู้ทั้งสองแบบกันว่าต่างกันอย่างไร</p>
<p>ใน Machine learning algorithm ทั่วไป เช่น Linear regression และ logistic regression โครงสร้างการเรียนรู้เป็นดังนี้:</p>
<p><img alt="Classical ML learning architecture" src="images/ml-blog13-01.png"></p>
<p>จะเห็นว่ามี 3 ส่วน คือ Input, Classifier, และ Prediction</p>
<p>1) Input คือข้อมูลที่ป้อนเข้า Algorithm เช่นในกรณีนี้มี 3 Feature ได้แก่ <script type="math/tex">x_1</script>, <script type="math/tex">x_2</script>, และ <script type="math/tex">x_3</script>
</p>
<p>2) Classifier คือฟังก์ชันที่นำเอา Input <script type="math/tex">x</script> มาคำนวนร่วมกับค่าน้ำหนัก <script type="math/tex">w</script> ของ <script type="math/tex">x</script> แต่ละตัว ผลที่ได้เรียกว่า <script type="math/tex">z</script> แล้วป้อน <script type="math/tex">z</script> เข้าสู่ฟังก์ชันตัดสินใจ <script type="math/tex">\sigma</script> เช่น Sigmoid function เพื่อคำนวนความเป็นไปได้ของคำตอบ</p>
<p>3) Prediction คือการตีความคำตอบของ Classifier แล้วนำเสนอผลลัพธ์ให้ผู้ใช้ เช่นคำถาม Binary classification ก็ให้คำตอบว่า "ใช่" หรือ "ไม่ใช่"</p>
<p>ในขณะที่ Neural network มีโครงสร้างการเรียนรู้ดังนี้:</p>
<p><img alt="Neural network learning architecture" src="images/ml-blog13-02.png"></p>
<p>จะเห็นว่ามี 4 ส่วน คือ Input layer, Hidden layer, Output layer, และ Prediction โดยเราจะอธิบายการทำงานทีละส่วน อนึ่ง เราเรียกกระบวนการที่ทั้ง 4 ส่วนนี้ทำงานต่อเนื่องกันจากด้านซ้ายไปด้านขวาของโครงสร้าง Neural network ว่ากระบวนการแผ่กระจายเดินหน้า หรือ Forward propagation ซึ่งทำงานร่วมกับการแผ่กระจายย้อนกลับ หรือ Backward propagation ซึ่งจะอธิบายภายหลัง</p>
<p>แต่ก่อนที่จะรู้จัก Forward propagation หลายคนอาจจะเคยได้ยินหรือเคยตั้งคำถามว่า Artificial neural network นี้มีความเหมือนหรือต่างกับการทำงานของสมองหรือไม่อย่างไร เราลองมาดูภาพ Neuron ในสมอง ซึ่งเป็นหน่วยที่รับ ประมวลผล และส่งข้อมูลไปยังส่วนอื่นๆ:</p>
<p><img alt="Brain neuron" src="images/ml-blog13-brain-neuron.png"></p>
<p>ภาพจาก <a href="https://commons.wikimedia.org/w/index.php?curid=28761830">BruceBlaus</a> - Own work, CC BY 3.0</p>
<p>หากจะเปรียบเทียบ โครงสร้างที่เห็นคือ Neuron ซึ่งรับข้อมูลมาจาก Dendrite แล้วส่งข้อมูลออกไปยัง Neuron ตัวถัดไปผ่าน Axon โดย Neuron แต่ละตัวจะรับและส่งข้อมูลกับ Neuron อื่นๆ หลายตัว ปัจจุบันเรายังไม่มีความเข้าใจดีนักว่าโครงสร้างนี้มีความหมายต่อกระบวนการเรียนรู้ของมนุษย์อย่างไร แต่ถ้าจะให้คาดเดา ก็ดูเหมือนว่าโครงสร้างที่มีการเชื่อมโยงกันเป็นเครือข่ายแบบนี้น่าจะทำให้สมองมีความสามารถในการประมวลผลข้อมูลที่รับมา กลายเป็นการตัดสินใจทั้งระดับร่างกายและจิตใจได้ Artificial neural network ได้รับแรงบันดาลใจมาจากโครงสร้างนี้ ถึงแม้กลไกการทำงานไม่จำเป็นต้องเหมือนกันก็ตาม</p>
<h2>Forward propagation</h2>
<p>สมมุติว่า Algorithm มี Parameter <script type="math/tex">w</script> และ <script type="math/tex">b</script> ที่เป็นตัวแทนของข้อมูลเรียบร้อยแล้ว กระบวนการ Forward propagation คือการนำข้อมูล <script type="math/tex">x</script> เข้ามาประมวลผลร่วมกับ Parameter เหล่านั้นเป็นชั้นๆ จนได้คำตอบ</p>
<p>อย่างไรก็ตาม ในตอนเริ่มต้น โมเดลจะยังไม่มี Parameter ที่ถูกต้อง เราจึงต้องสุ่มค่าเริ่มต้นของ Parameter ขึ้นมาก่อน เมื่อ Forward propagation ทำงานจบ 1 เที่ยว ก็จะเปรียบเทียบผลการพยากรณ์กับคำตอบที่รู้อยู่แล้ว จากนั้นโมเดลจะใช้กระบวนการตรงกันข้าม คือ Backward propagation ในการปรับค่า Parameter ให้สะท้อนข้อมูลใน Train set มากขึ้น ทำอย่างนี้หลายๆ รอบจนกระทั่งได้ความแม่นยำของโมเดลตามที่ต้องการ อนึ่งเราเรียกการทำงานไป-กลับแต่ละรอบว่า 1 Epoch (ภาษาไทยอาจแปล Epoch ว่า "กัป" ซึ่งฟังดูยิ่งใหญ่ดี แต่อาจจะไม่เหมาะกับบริบทนี้ จึงขอใช้ Epoch คำเดิม)</p>
<p>Forward propagation มีกระบวนการดังนี้:</p>
<p>1) Input layer คือข้อมูลขาเข้า ถ้าเป็นข้อมูลแบบมีโครงสร้าง <script type="math/tex">x</script> แต่ละตัวจะแทน Feature หรือคอลัมน์ของข้อมูล เช่น อายุ เพศ รายได้ เป็นต้น</p>
<p>แต่ถ้าเป็นข้อมูลแบบไม่มีโครงสร้าง เช่นรูปภาพ <script type="math/tex">x</script> แต่ละตัวจะแทนค่าความสว่างของภาพ 1 Pixel ดังนั้น ถ้าเรามีภาพขาวดำขนาด 64 คูณ 64 Pixel เราจะได้จำนวน Input <script type="math/tex">m = 64 \times 64 = 4096</script> แต่ถ้าเป็นภาพสี ต้องคูณ 3 เข้าไป เพราะภาพสีแต่ละ Pixel จะมีข้อมูลความสว่าง 3 สี คือ Red, Green, Blue</p>
<p>2) Hidden layer คือชั้นประมวลผลที่ซ่อนอยู่ ซึ่งมีได้หลายชั้น (จากตัวอย่างแสดงชั้นเดียว) ใน Hidden layer แต่ละชั้นจะมีหน่วยประมวลผลที่เรียกว่า Neuron โดยในภาพมี 4 Neuron</p>
<p>หน้าที่ของ Neuron แต่ละตัว คือการรับข้อมูล Input "ทุกตัว" จาก Layer ก่อนหน้า มาประมวลผลโดยใช้ Linear function ร่วมกับค่าน้ำหนัก <script type="math/tex">w</script> ของ Input แต่ละตัว ซึ่งให้ผล <script type="math/tex">z</script> แล้วนำ <script type="math/tex">z</script> ไปคำนวนใน Activation function <script type="math/tex">g</script> ซึ่งอาจจะเป็น Sigmoid, Tanh, หรือ RELU ก็ได้ คำตอบที่ได้เรียกว่า <script type="math/tex">a</script> สังเกตว่ากระบวนการนี้คล้ายกับขั้นตอน Classifier ของ ML algorithm ทั่วไป สิ่งที่ต่างคือชื่อเรียก สัญลักษณ์ที่ใช้แทน และจำนวน Neuron ที่มีมากกว่า 1</p>
<p>เราสามารถเขียนขั้นตอนนี้เป็นสมการได้ดังนี้:</p>
<p><em>สำหรับ <script type="math/tex">x^{(i)}</script> แต่ละรายการ:</em></p>
<p>
<script type="math/tex; mode=display">z^{[1](i)} = W^{[1]}x^{(i)} + b^{[1]} \tag{1}</script>
</p>
<p>
<script type="math/tex; mode=display">a^{[1](i)} = g(z^{[1](i)}) \tag{2}</script>
</p>
<p>โดยก่อนจะไปไกลกว่านี้ มาทำความคุ้นเคยกับสัญลักษณ์แทนลำดับที่ของสิ่งต่างๆ ในโมเดลกันก่อน:</p>
<ul>
<li>
<script type="math/tex">[1]</script> ใน Brackets หมายถึงลำดับที่ของ Layer โดยนับ 1 ที่ Hidden layer แรก</li>
<li>
<script type="math/tex">(i)</script> ใน Parentheses หมายถึงลำดับรายการข้อมูลที่ <script type="math/tex">i</script>
</li>
<li>
<script type="math/tex">1</script> ที่ห้อยด้านล่าง <script type="math/tex">a</script> คือลำดับที่ของ Neuron ใน Layer นั้นๆ</li>
<li>นำสัญลักษณ์ทั้งหมดมารวมกัน ตัวอย่างเช่น <script type="math/tex">a^{[1](4)}_3</script> หมายถึง Activation function ตัวที่ 3 ของ Layer ที่ 1 ที่ทำงานกับข้อมูลรายการที่ 4</li>
</ul>
<p>สิ่งสำคัญที่ต้องรู้ในขั้นนี้ คือ Hidden layer สามารถมีได้หลายชั้น เช่นถ้าหากมีชั้นที่ 2 สมการที่ (1) ก็จะนำ <script type="math/tex">a^{[1]}</script> มาแทน <script type="math/tex">x</script> และใช้ <script type="math/tex">W</script> และ <script type="math/tex">b</script> ของ Layer ที่สอง ดังนั้นเราสามารถเปลี่ยนรูปสมการที่ (1) และ (2) ให้เป็นรูปทั่วไปได้ดังนี้ (Vectorised form):</p>
<p>
<script type="math/tex; mode=display">Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \tag{3}</script>
</p>
<p>
<script type="math/tex; mode=display">A^{[l]} = g(Z^{[l]}) \tag{4}</script>
</p>
<p>โดย <script type="math/tex">[l]</script> คือลำดับที่ของ Layer ซึ่งนับ 1 ที่ Hidden layer ที่ 1</p>
<p>คำว่า Deep ใน Deep learning ก็มาจากการที่โมเดลมี Layer หลายชั้นในการประมวลผลนั่นเอง</p>
<p>3) Output layer คือชั้นที่ประมวลผล Activation <script type="math/tex">a</script> ทั้งหมดจากชั้นก่อนหน้า โดยถือว่า <script type="math/tex">a^{[1]}</script> คือ Input ร่วมกับค่าน้ำหนัก <script type="math/tex">W^{[2]}</script> ของ <script type="math/tex">a^{[1]}</script> ในชั้นก่อนหน้า (ไม่ใช่ของ <script type="math/tex">x</script> ในชั้นแรก) ได้ผลเป็น <script type="math/tex">z^{[2]}</script> แล้วนำ <script type="math/tex">z^{[2]}</script> ไปคำนวนใน Activation function เช่น Sigmoid function ได้ผลเป็น <script type="math/tex">a^{[2]}</script>
</p>
<p>เขียนเป็นสมการได้ดังนี้ (สังเกตว่าสอดคล้องกับสมการที่ (3) และ (4)):</p>
<p>
<script type="math/tex; mode=display">z^{[2](i)} = W^{[2]}a^{[1](i)} + b^{[2]} \tag{5}</script>
</p>
<p>
<script type="math/tex; mode=display">a^{[2](i)} = \sigma(z^{[2](i)}) \tag{6}</script>
</p>
<p>หรือเขียนเป็นรูปทั่วไป (Vectorised) คือ:</p>
<p>
<script type="math/tex; mode=display">Z^{[L]} = W^{[L]}A^{[L-1]} + b^{[L]} \tag{7}</script>
</p>
<p>
<script type="math/tex; mode=display">A^{[L]} = \sigma(Z^{[L]}) \tag{8}</script>
</p>
<p>โดย <script type="math/tex">[L]</script> คือจำนวน Layer ทั้งหมด ดังนั้นจึงหมายถึง Layer สุดท้าย ส่วน <script type="math/tex">[L-1]</script> ก็คือ Layer รองสุดท้าย</p>
<p>เพื่อความเข้าใจที่ดีขึ้น จะนำเสนอให้ดูว่าข้อมูลและตัวแปรแต่ละตัวใน Neural network มีหน้าตาอย่างไรในรูปแบบ Matrix และ Vector</p>
<p>สมมุติว่าเรามีข้อมูลดังนี้: Feature <script type="math/tex">n_0 = 4096</script> จำนวน <script type="math/tex">m = 10000</script> รายการ:</p>
<p>
<script type="math/tex; mode=display">X =
\begin{pmatrix}
    x_1^{(1)} & x_1^{(2)} & x_1^{(3)} & \cdots & x_1^{(10000)} \\
    x_2^{(1)} & x_2^{(2)} & x_2^{(3)} & \cdots & x_2^{(10000)} \\
    x_3^{(1)} & x_3^{(2)} & x_3^{(3)} & \cdots & x_3^{(10000)} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{4096}^{(1)} & x_{4096}^{(2)} & x_{4096}^{(3)} & \cdots & x_{4096}^{(10000)} \\
\end{pmatrix}
\tag{9}</script>
</p>
<p>มิติของ Matrix X คือ <script type="math/tex">(n_0, m) = (4096, 10000)</script>
</p>
<p>เราจะใช้ข้อมูลชุดนี้ใน Neural network ขนาด Layer <script type="math/tex">L = 2</script> โดยมีจำนวน Neuron ใน Layer แรก <script type="math/tex">n_1 = 4</script> Neuron ส่วน Layer ที่สองเป็น Output layer ดังนั้นจึงมี <script type="math/tex">n_2 = 1</script>
</p>
<p>ค่าน้ำหนัก <script type="math/tex">W^{[1]}</script> จะต้องคูณแบบ Dot product กับ <script type="math/tex">X</script> ได้ ดังนั้นจึงมีมิติ <script type="math/tex">(n_1, n_0) = (4, 4096)</script>:</p>
<p>
<script type="math/tex; mode=display">W^{[1]} =
\begin{pmatrix}
    w_1^{[1](1)} & w_1^{[1](2)} & w_1^{[1](3)} & \cdots & w_1^{[1](4096)} \\
    w_2^{[1](1)} & w_2^{[1](2)} & w_2^{[1](3)} & \cdots & w_2^{[1](4096)} \\
    w_3^{[1](1)} & w_3^{[1](2)} & w_3^{[1](3)} & \cdots & w_3^{[1](4096)} \\
    w_4^{[1](1)} & w_4^{[1](2)} & w_4^{[1](3)} & \cdots & w_4^{[1](4096)} \\
\end{pmatrix}
\tag{10}</script>
</p>
<p>เมื่อ <script type="math/tex">W^{[1]}X</script> อยู่ในมิติ <script type="math/tex">(n_1, n_0) \times (n_0, m) = (n_1, m) = (4, 10000)</script> ดังนั้น <script type="math/tex">b^{[1]}</script> จึงจะต้องอยู่ในมิติ <script type="math/tex">(n_1, 1) = (4, 1)</script> จึงจะสามารถบวกเข้าไปสมการได้:</p>
<p>
<script type="math/tex; mode=display">b^{[1]} =
\begin{pmatrix}
    b_1^{[1](1)} \\
    b_2^{[1](1)} \\
    b_3^{[1](1)} \\
    b_4^{[1](1)} \\
\end{pmatrix}
\tag{11}</script>
</p>
<p>เราต้องการ <script type="math/tex">Z^{[1]} = W^{[1]}X + b^{[1]}</script> ดังนั้น <script type="math/tex">Z^{[1]}</script> จึงต้องมีมิติ <script type="math/tex">(n_1, m) = (4, 10000)</script>:</p>
<p>
<script type="math/tex; mode=display">Z^{[1]} =
\begin{pmatrix}
    z_1^{[1](1)} & z_1^{[1](2)} & z_1^{[1](3)} & \cdots & z_1^{[1](10000)} \\
    z_2^{[1](1)} & z_2^{[1](2)} & z_2^{[1](3)} & \cdots & z_2^{[1](10000)} \\
    z_3^{[1](1)} & z_3^{[1](2)} & z_3^{[1](3)} & \cdots & z_3^{[1](10000)} \\
    z_4^{[1](1)} & z_4^{[1](2)} & z_4^{[1](3)} & \cdots & z_4^{[1](10000)} \\
\end{pmatrix}
\tag{12}</script>
</p>
<p>ส่วน <script type="math/tex">A^{[1]}</script> ก็มีมิติ <script type="math/tex">(n_1, m)</script> เหมือน <script type="math/tex">Z^{[1]}</script> เพราะเป็นการนำ <script type="math/tex">Z^{[1]}</script> มา Apply activation function ด้วยการ Broadcast ฟังก์ชันเข้าไปใน Matrix:</p>
<p>
<script type="math/tex; mode=display">A^{[1]} =
\begin{pmatrix}
    g(z_1^{[1](1)}) & g(z_1^{[1](2)}) & g(z_1^{[1](3)}) & \cdots & g(z_1^{[1](10000)}) \\
    g(z_2^{[1](1)}) & g(z_2^{[1](2)}) & g(z_2^{[1](3)}) & \cdots & g(z_2^{[1](10000)}) \\
    g(z_3^{[1](1)}) & g(z_3^{[1](2)}) & g(z_3^{[1](3)}) & \cdots & g(z_3^{[1](10000)}) \\
    g(z_4^{[1](1)}) & g(z_4^{[1](2)}) & g(z_4^{[1](3)}) & \cdots & g(z_4^{[1](10000)}) \\
\end{pmatrix}
\tag{13}</script>
</p>
<p>4) Prediction นำเอาผล Activation <script type="math/tex">a^{[2]}</script> ของ Output layer มาตัดสินใจใน Decision function เพื่อพยากรณ์ เขียนเป็นสมการได้ดังนี้:</p>
<p>
<script type="math/tex; mode=display">\hat{y}^{(i)} = a^{[2](i)} \tag{14}</script>
</p>
<p>
<script type="math/tex; mode=display">y^{(i)}_{predict} =
\begin{cases}
    1 \quad \text{if } a^{[2](i)} \geq 0.5 \\
    0 \quad \text{if } a^{[2](i)} < 0.5
\end{cases} \tag{15}</script>
</p>
<p>ย้อนกลับมายังคำถามตั้งต้น ว่าทำไม Neural network จึงทำงานได้ดีกับข้อมูลแบบที่ไม่มีโครงสร้าง ลองพิจารณาดังนี้:</p>
<ul>
<li>ข้อมูลที่ไม่มีโครงสร้าง เช่นภาพ ข้อความ เสียง จะถูกนำเสนอในโมเดล ในลักษณะที่เป็น Feature จำนวนมาก โดยแต่ละ Feature แทนค่าหน่วยของข้อมูลที่บ่งบอกลักษณะของข้อมูลชิ้นนั้น เช่นความสว่างของ Pixel แต่ละจุด การนำเสนอแบบนี้ทำให้มนุษย์ไม่สามารถบอกหรือเตรียมล่วงหน้าได้ว่า Feature ไหนมีความสำคัญมากน้อย เพราะมีจำนวนมากและมีความซับซ้อนเกินกว่าที่สมองมนุษย์จะประมวลผลได้ ไม่เหมือน Feature ของข้อมูลที่มีโครงสร้างที่อยู่ในรูปของคอลัมน์ในตาราง ที่เราสามารถเตรียมเพิ่ม ลด ปรับแต่ง Feature เพื่อให้มีความหมายต่อโมเดล</li>
<li>เมื่อเป็นเช่นนี้ หากเราใช้ Algorithm ทั่วไปในการเทรนข้อมูลที่มี Feature จำนวนมหาศาลแบบนี้ ก็มักจะได้ผลที่ไม่ดี เช่นเกิดปัญหา Variance ซึ่งก็คือการที่ Algorithm พยายามฟิตข้อมูลเข้ากับคำตอบที่รู้อยู่แล้ว แต่ Parameter เดียวกันนี้ไม่สามารถนำไปทำนายข้อมูลใหม่ที่มองไม่เห็นได้ดี เพราะจำนวนและความซับซ้อนของ Feature นั้นเกินกำลังที่ Algorithm ทั่วไปที่คำนวนชั้นเดียวจะสามารถทำความเข้าใจได้ดี</li>
<li>ในขณะที่ Neural network จะแบ่งลำดับขั้นในการเรียนรู้ออกเป็นชั้นๆ โดยชั้นแรกจะเรียนรู้เพื่อแยกแยะ Feature อย่างหยาบๆ เช่น ถ้าเป็นภาพ ก็จะแยกบริเวณที่มีค่าความต่างของสีมาก เช่น ขอบของวัตถุ ออกมาก่อน จากนั้นจึงส่งข้อมูลที่แยกแยะเบื้องต้นแล้ว ไปยัง Layer ชั้นถัดไป ซึ่งจะได้รับข้อมูลที่ประมวลมาแล้ว นำไปแยกแยะค่าน้ำหนักของส่วนที่มีความละเอียดมากขึ้น เช่น ส่วนต่างๆ ของใบหน้า เป็นต้น</li>
<li>การแยกแยะลักษณะของข้อมูลในแต่ละ Layer อาศัย Neuron หลายๆ ตัวทำงานพร้อมกัน โดยแต่ละตัวจะรับข้อมูลทั้งหมดจากชั้นที่แล้วเหมือนกัน แต่ให้ค่าน้ำหนัก <script type="math/tex">w</script> ไม่เท่ากัน การที่ค่าน้ำหนักของข้อมูล <script type="math/tex">x</script> แต่ละชิ้นไม่เท่ากันเลย ส่งผลเท่ากับการที่ Neuron แต่ละตัวช่วยกันประมวลข้อมูลเดียวกันโดยใช้มุมมองที่ต่างกันไปเล็กน้อย ทำให้เกิดผลเป็นการ Regularise ให้ข้อมูลลดโอกาสที่จะ Overfit train set ไปโดยปริยาย</li>
</ul>
<p>ทบทวนอีกครั้งว่ากระบวนการทั้งหมดนี้ เรียกว่า Forward propagation ซึ่งจบลงด้วยการได้ค่าพยากรณ์ แต่แน่นอนว่าเมื่อเรายังไม่มี Parameter <script type="math/tex">w</script> และ <script type="math/tex">b</script> ที่ถูกต้อง ค่าที่พยากรณ์ได้ก็จะไม่ตรงกับความจริง ดังนั้นเราจะใช้กระบวนการ Backward propagation ในการปรับแต่ง Parameter ให้เป็นตัวแทนของข้อมูลได้เที่ยงตรงยิ่งขึ้น</p>
<h2>Backward propagation</h2>
<p>บางคนอาจจะเคยได้ยินว่า Backward propagation คือส่วนที่ซับซ้อนและยากที่สุดของ Neural network algorithm ซึ่งอาจจะจริงในส่วนของการคำนวน แต่โดยหลักคิดแล้วไม่ได้ยากขนาดนั้น เรามาลองทำความเข้าใจกันดู</p>
<p>ทบทวนว่า กระบวนการ Forward propagation จบลงที่เราได้ค่าพยากรณ์ ดังนั้นสิ่งต่อไปที่เราต้องทำ คือการนำค่าพยากรณ์นั้นมาใส่ใน Cost function เพื่อหาความต่างระหว่างค่าพยากรณ์กับค่าจริง โดยสำหรับ Neural network เราจะใช้ Cost function ลักษณะเดียวกันกับ Logistic regression คือ:</p>
<p>สมมุติว่า <script type="math/tex">L = 2</script>:</p>
<p>
<script type="math/tex; mode=display">J = -\frac{1}{m} \sum\limits_{i=0}^{m} \left( y^{(i)} \log(a^{[2](i)}) + (1-y^{(i)}) \log(1-a^{[2](i)}) \right) \tag{16}</script>
</p>
<p>อนึ่ง หากใครสนใจ เราสามารถเขียนโค้ดของ Cost function โดยใช้ numpy ช่วย Vectorise เพื่อให้สามารถคำนวน Cost function ของรายการข้อมูลทั้งหมดได้อย่างรวดเร็วโดยไม่ต้องใช้ For loop โดยเขียนได้ดังนี้:</p>
<pre><code>logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2),1-Y)
cost = (-1/m)*np.sum(logprobs)
</code></pre>

<p>สิ่งต่อไปที่เราจะทำ คือเราจะหาว่า Parameter ที่ทำให้ Cost function มีค่าต่ำที่สุด โดยใช้กระบวนการ Gradient descent ซึ่งมีหลักการคือ:</p>
<p>1) หาอนุพันธ์ของ Parameter เช่น <script type="math/tex">w</script> เมื่อเปรียบเทียบกับ Cost function <script type="math/tex">J</script>:</p>
<p>
<script type="math/tex; mode=display">\frac{\partial J(w)}{\partial w}\tag{17}</script>
</p>
<p>2) นำอนุพันธ์ที่ได้ไปลบออกจาก Parameter นั้น โดยควบคุมความเร็วในการลบด้วย Learning rate <script type="math/tex">\alpha</script> แล้วนำ Parameter ใหม่ไปคำนวนใน Forward propagation จะได้ Cost function ที่มีค่าลดลง ทำซ้ำขั้นตอนนี้ไปเรื่อยๆ จน Cost function มีค่าต่ำที่สุดที่จะเป็นไปได้:</p>
<p><em>ทำซ้ำจนกระทั่งผลลัพธ์ล่าสุดไม่เปลี่ยนแปลงจากผลลัพธ์ครั้งก่อน:</em></p>
<p>
<script type="math/tex; mode=display">w := w - \alpha \frac{\partial}{\partial{w}} J(w)\tag{18}</script>
</p>
<p>กระบวนการดังกล่าวแสดงเป็นภาพได้ดังนี้:</p>
<p><img alt="Gradient descent" src="images/ml-blog02-03.png"></p>
<p>ที่อธิบายไป เป็นภาพรวมของกระบวนการ แต่ใน Neural network เรามี Parameter หลายตัวและหลายชั้น ดังนั้นเราจึงต้องใช้หลายสมการในการหาอนุพันธ์ของ Parameter แต่ละตัว</p>
<p>โดย Parameter ที่ต้องหาอนุพันธ์ คือ Parameter ที่ส่งผลต่อค่าพยากรณ์ ได้แก่:</p>
<ul>
<li>
<script type="math/tex">z</script> ของแต่ละ Layer ไม่ได้เป็นตัวแปรที่ต้องการ Optimise แต่จำเป็นต้องหาอนุพันธ์เพื่อจะได้ใช้ Chain rule หาอนุพันธ์ของ <script type="math/tex">w</script> และ <script type="math/tex">b</script> ได้</li>
<li>
<script type="math/tex">w</script> ของแต่ละ Layer เพราะเป็น Coefficient ของ <script type="math/tex">x</script>
</li>
<li>
<script type="math/tex">b</script> ของแต่ละ Layer เพราะเป็น Intercept ของ Linear function <script type="math/tex">z = wx + b</script>
</li>
</ul>
<p>จะไม่แสดงวิธีการหาอนุพันธ์ของ Parameter แต่ละตัว เพราะยุ่งยากซับซ้อนค่อนข้างมาก แต่จะแสดงให้เห็นเลยว่าอนุพันธ์ของแต่ละ Parameter คืออะไร:</p>
<p>สมมุติว่า <script type="math/tex">L = 2</script>:</p>
<p>
<script type="math/tex; mode=display">\color{blue}{dz^{[2]}} = a^{[2]}-y \tag{19}</script>
</p>
<p>
<script type="math/tex; mode=display">dW^{[2]} = \color{blue}{dz^{[2]}} a^{[1]T}\tag{20}</script>
</p>
<p>
<script type="math/tex; mode=display">db^{[2]} = \color{blue}{dz^{[2]}} \tag{21}</script>
</p>
<p>
<script type="math/tex; mode=display">\color{red}{dz^{[1]}} = W^{[2]T} \color{blue}{dz^{[2]}} \tag{22}</script>
</p>
<p>
<script type="math/tex; mode=display">dW^{[1]} = \color{red}{dz^{[1]}} x^T \tag{23}</script>
</p>
<p>
<script type="math/tex; mode=display">db^{[1]} = \color{red}{dz^{[1]}} \tag{24}</script>
</p>
<p>ทั้งนี้จะไม่แสดงวิธีคิดแบบ Vectorised เพราะอยากเน้นให้เข้าใจหลักการมากกว่า ส่วนในทางปฏิบัติให้ใช้ Framework อย่าง Tensorflow จะสะดวกกว่ามาก</p>
<p>สังเกตว่าเราหาอนุพันธ์ของ <script type="math/tex">z</script> เช่น <script type="math/tex">\color{blue}{dz^{[2]}}</script> และ <script type="math/tex">\color{red}{dz^{[1]}}</script> เพื่อเป็นอนุพันธ์ตั้งต้นให้หาอนุพันธ์ของ <script type="math/tex">w</script> และ <script type="math/tex">b</script> ได้ตามที่เขียนไว้ข้างต้น</p>
<p>เมื่อเราได้อนุพันธ์ทั้ง 6 ตัว (สำหรับโมเดลความลึก 2 ชั้น ถ้า 3 ชั้นก็ต้องเพิ่มอีก 3 ตัว) เราก็จะเอาอนุพันธ์ของ <script type="math/tex">w</script> และ <script type="math/tex">b</script> ไปอัปเดตค่าตัวแปรทั้งสอง โดยทำดังนี้:</p>
<p>
<script type="math/tex; mode=display">W^{[1]} := W^{[1]} - \alpha (dW^{[1]}) \tag{25}</script>
</p>
<p>
<script type="math/tex; mode=display">b^{[1]} := b^{[1]} - \alpha (db^{[1]}) \tag{26}</script>
</p>
<p>
<script type="math/tex; mode=display">W^{[2]} := W^{[2]} - \alpha (dW^{[2]}) \tag{27}</script>
</p>
<p>
<script type="math/tex; mode=display">b^{[2]} := b^{[2]} - \alpha (db^{[2]}) \tag{28}</script>
</p>
<p>แล้วนำตัวแปรที่อัปเดตแล้วไปคำนวน Forward propagation ใน Epoch ใหม่ แล้วคิดอนุพันธ์ นำอนุพันธ์มาอัปเดตตัวแปร ทำอย่างนี้ซ้ำไปเรื่อยๆ จนถึงจุดที่ Cost function มีค่าต่ำที่สุด ก็จะได้โมเดลที่ฟิตกับ Train set ที่ดีที่สุดเท่าที่จะเป็นไปได้</p>
<p>มาถึงจุดนี้ ก็คงพอเข้าใจแล้วว่า Neural network ทำงานอย่างไร และเรียนรู้อย่างไร เรื่อง Neural network นี้มีรายละเอียดและวิธีการประยุกต์ใช้มากมาย ซึ่งจะค่อยๆ อธิบายในบทอื่นๆ</p>
<p>ส่วนในบทต่อไป เราจะเริ่มทดลองสร้างโมเดล Neural network และพยากรณ์ โดยใช้ Framework อย่าง Tensorflow และ Keras</p>
<p><a href="index.html">หน้าแรก</a> | <a href="ml-blog12.html">บทที่ 12 Clustering</a> | <a href="ml-blog14.html">บทที่ 14 Neural Network Programming</a></p>
<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
</body>
</html>
